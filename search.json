[
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "1 + 1 = 2 ‚Ä¶ perhaps not for Microsoft Copilot",
    "section": "",
    "text": "For most people, counting numbers is a simple operation composed of basically adding numbers. From an early age, most of us know how to compute 1 + 1, 2 + 2, and 2 - 1. It is an intuitive operation that is ubiquitous in our daily lives. As LLMs are basically developed to generate responses that are most likely human ones, one would infer that LLMs do not have a problem in computing the total price of multiple objects, calculating means and medians, and counting the number of letters in a single word‚Ä¶ However, this last one might not be so easy for them to figure out. To better understand what I am saying, let‚Äôs go through a sample question\n\nCount the number that each letter appears in the word college.\n\n\nYou will probably answer something like c = 1, o = 1, l = 2, e = 2, g = 1‚Ä¶\nAnd that‚Äôs it! There is no interpretation, no magical formula, no advanced counting theorem. Just pure and simple math. However, how do we really perform this operation?\n\n\nSome people might deliberate a bit on these simple additions, especially people who might instantly respond to the questions. No matter if it is our automatic or analytical brain system (shoutout to Daniel Kahneman) that is performing the operation, one thing is for sure: it is a simple counting operation.\nWe basically identify different characters visually based on previous knowledge of the alphabet and its characters and perform logical steps composed of adding numbers to check how many times the characters with the same visual representation are represented in this specific range of letters. This ability to perform simple operations (which are not too simple for a machine) comes from our brain‚Äôs impressive capacity for interpreting, storing, and using visual data for other means. It is as simple as that: you look, interpret, and do whatever you want with this information without any additional step to evaluate such values. However, that is not how LLMs work.\nTo perform any type of operation, LLMs rely on a process called tokenization. Tokenization is one of the fundamental steps in data analysis for LLMs in which they process, usually words, as a set of integers. For instance, while we read ‚Äúdog‚Äù as just ‚Äúdog,‚Äù a machine interprets dogs as 001. This step is pivotal for generative AI, as it allows the machine to make associations of what words most often appear after the dog in a sentence and, thus, allows it to come up with a plausible answer.\nBut letters are a whole different story‚Ä¶ You might be wondering, what if we want to analyze letters in a word? This tokenization process is then applied to each individual character in the designated word, which turns out to be troublesome for some specific LLMs. In the case of Copilot, no matter if it is a common word in terms of recurrence or an unusual word, it has difficulties in tokenizing it. Check it out below.\n\n\n\nGPT3.5 Turbo\n\n\nAs you can see, it has a lot of difficulties processing the word anthropology.\n\nWhat about other words\n\n\nThis time, though, Copilot successfully counted the number of letters in the word college\n\nIs is the same thing for ChatGPT 4?\n\n\nUnlike what you may be thinking, not actually. Supposedly, Copilot uses Open AI‚Äôs GPT 4, too. However, when we insert the exact same prompt on ChatGPT 4, it answers correctly!\n\nWhat can we conclude with this experiment?\nI would say that the results obtained can show us two important things: LLMs are still, in general, not good at processing individual words, and ChatGPT 4 and Copilot are perhaps not as similar as we thought.\nRegarding the first assertion, doing this experience makes it really easy to visualize the limitations of performing tokenization in comparison to the way that the human brain interprets words and counting. Probably, for more unusual words, like anthropology, Copilot simply attributes the same tokens to different letters or computes syllables as single words. Meanwhile, for more usual words in the English language, like college, its word tokenization process is more optimized and thus does not generate any problem in terms of interpretation and computation.\nWHAT ABOUT ChatGPT 4?\nWell, that is content for other articles, but at least in theory, Copilot should behave similarly to ChatGPT 4. Perhaps this indicates that, in fact, ChatGPT 4 and Copilot are not as similar as Microsoft proposed‚Ä¶\nThank you for reading!"
  },
  {
    "objectID": "posts/000_test_post/index.html#a-simple-counting-problem",
    "href": "posts/000_test_post/index.html#a-simple-counting-problem",
    "title": "1 + 1 = 2 ‚Ä¶ perhaps not for Microsoft Copilot",
    "section": "",
    "text": "For most people, counting numbers is a simple operation composed of basically adding numbers. From an early age, most of us know how to compute 1 + 1, 2 + 2, and 2 - 1. It is an intuitive operation that is ubiquitous in our daily lives. As LLMs are basically developed to generate responses that are most likely human ones, one would infer that LLMs do not have a problem in computing the total price of multiple objects, calculating means and medians, and counting the number of letters in a single word‚Ä¶ However, this last one might not be so easy for them to figure out. To better understand what I am saying, let‚Äôs go through a sample question\n\nCount the number that each letter appears in the word college.\n\n\nYou will probably answer something like c = 1, o = 1, l = 2, e = 2, g = 1‚Ä¶\nAnd that‚Äôs it! There is no interpretation, no magical formula, no advanced counting theorem. Just pure and simple math. However, how do we really perform this operation?\n\n\nSome people might deliberate a bit on these simple additions, especially people who might instantly respond to the questions. No matter if it is our automatic or analytical brain system (shoutout to Daniel Kahneman) that is performing the operation, one thing is for sure: it is a simple counting operation.\nWe basically identify different characters visually based on previous knowledge of the alphabet and its characters and perform logical steps composed of adding numbers to check how many times the characters with the same visual representation are represented in this specific range of letters. This ability to perform simple operations (which are not too simple for a machine) comes from our brain‚Äôs impressive capacity for interpreting, storing, and using visual data for other means. It is as simple as that: you look, interpret, and do whatever you want with this information without any additional step to evaluate such values. However, that is not how LLMs work.\nTo perform any type of operation, LLMs rely on a process called tokenization. Tokenization is one of the fundamental steps in data analysis for LLMs in which they process, usually words, as a set of integers. For instance, while we read ‚Äúdog‚Äù as just ‚Äúdog,‚Äù a machine interprets dogs as 001. This step is pivotal for generative AI, as it allows the machine to make associations of what words most often appear after the dog in a sentence and, thus, allows it to come up with a plausible answer.\nBut letters are a whole different story‚Ä¶ You might be wondering, what if we want to analyze letters in a word? This tokenization process is then applied to each individual character in the designated word, which turns out to be troublesome for some specific LLMs. In the case of Copilot, no matter if it is a common word in terms of recurrence or an unusual word, it has difficulties in tokenizing it. Check it out below.\n\n\n\nGPT3.5 Turbo\n\n\nAs you can see, it has a lot of difficulties processing the word anthropology.\n\nWhat about other words\n\n\nThis time, though, Copilot successfully counted the number of letters in the word college\n\nIs is the same thing for ChatGPT 4?\n\n\nUnlike what you may be thinking, not actually. Supposedly, Copilot uses Open AI‚Äôs GPT 4, too. However, when we insert the exact same prompt on ChatGPT 4, it answers correctly!\n\nWhat can we conclude with this experiment?\nI would say that the results obtained can show us two important things: LLMs are still, in general, not good at processing individual words, and ChatGPT 4 and Copilot are perhaps not as similar as we thought.\nRegarding the first assertion, doing this experience makes it really easy to visualize the limitations of performing tokenization in comparison to the way that the human brain interprets words and counting. Probably, for more unusual words, like anthropology, Copilot simply attributes the same tokens to different letters or computes syllables as single words. Meanwhile, for more usual words in the English language, like college, its word tokenization process is more optimized and thus does not generate any problem in terms of interpretation and computation.\nWHAT ABOUT ChatGPT 4?\nWell, that is content for other articles, but at least in theory, Copilot should behave similarly to ChatGPT 4. Perhaps this indicates that, in fact, ChatGPT 4 and Copilot are not as similar as Microsoft proposed‚Ä¶\nThank you for reading!"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Explorations with LLMs",
    "section": "",
    "text": "1 + 1 = 2 ‚Ä¶ perhaps not for Microsoft Copilot\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\nCopilot\n\n\n\nA brief analysis of one of Microsoft Copilot weaknesses\n\n\n\n\n\nFeb 10, 2024\n\n\nGabriel\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\n\n\n\nLLMs\n\n\nprompting\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nFeb 2, 2024\n\n\nAn LLM User\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/000_test_post2/index.html",
    "href": "posts/000_test_post2/index.html",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  },
  {
    "objectID": "posts/000_test_post2/index.html#a-visualization-problem-for-llms",
    "href": "posts/000_test_post2/index.html#a-visualization-problem-for-llms",
    "title": "A test post",
    "section": "",
    "text": "Read the following and see if you can answer the question:\n\n\nThere are three boxes in a stack. A pink one, a purple one and a green one. The pink one is in the middle of the stack with the purple below it. What is the color of the box on the bottom of the stack?\n\nMost likely you answered purple‚Ä¶\nBut it is possible you first said green (don‚Äôt worry if you did it is quite a common response!)\n\n\nIt‚Äôs a verbal visual problem that requires some deliberation and most likely for us to create a visual image of a stack of boxes as a reasoning aid.\nNotice also the complexity of the language used in the prompt to describe the scene. The use of natural language as opposed to a formal/logical calculus does hold the potential for ambiguity (multiple interpretations) and inexactness. However, it is the most normal/natural, efficient and broadly accessible way of communicating such information.\nNotice 1. The use of anaphor or reference. - Three BOXES -&gt; a pink/purple/green ONE as opposed to a pink box, a purple box and a green box. - The PINK ONE is in the middle ... the purple {BOX} below IT {the pink box}\nWe tend to try and avoid this kind of repetition to make our speech more coherent.\n\nAlthough the description in language may not be entirely concise, there is no real ambiguity as to how the boxes can be arranged. It must be: | Box | Stack position | | ‚Äî | ‚Äî | | GREEN | top | | PINK | middle | | PURPLE | bottom |\n\n\n\n\nGPT3.5 Turbo\n\nHad some difficulties and answered green\nLlama2 (chat 7B version)\n\n\nAnswered correctly (purple) and used some so-called chain-of-thought reasoning\n\nBing Chat (MS Co-pilot)\nThis LLM is GPT4 with some Microsoft specific tuning and RAG (search) additions.\nIt get‚Äôs it correct:\n\nThe color of the box on the bottom of the stack is purple. This is because the purple box is below the pink one, which is in the middle of the stack. Therefore, the purple box must be the lowest one. üü™\n\nBut when you follow up and ask it to create visual representation of the box arrangement\n\nPROMPT: Can you create a visual representation of how the boxes are arranged?\n\nIt responds\n\nRESPONSE: Sure, I‚Äôll try to create that. Here is a visual representation of how the boxes are arranged:\n\nAnd uses DALL-E3 to create 4 different attempts at a visual representation:\n\nNONE OF WHICH ARE CORRECT!!!"
  }
]