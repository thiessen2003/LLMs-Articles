{
 "cells": [
  {
   "cell_type": "raw",
   "id": "ceb26b6d-d77d-4a43-8a72-f9758596005e",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Part 1 - LLMs filters and security restrictions\"\n",
    "description: \"Why doesn't GPT provide you with answers to every single prompt?\"\n",
    "author: \"Gabriel Thiessen\"\n",
    "date: \"2/25/2024\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - ChatGPT\n",
    "  - filter\n",
    "  - prompt hacking\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a71c0d0-759c-45ff-a3c4-e7a16f2a0339",
   "metadata": {},
   "source": [
    "<img src=\"stop.png\" width=\"100%\"/>\n",
    "Prompt: Draw an image of someone stopping ChatGPT from talking. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301cf3c8-8d93-4fcb-bc7f-d09fe749bae1",
   "metadata": {},
   "source": [
    "## Restrictions, restrictions, and restrictions\n",
    "\n",
    "Have you ever tried to find the correct formula to build a bomb at home? Have you ever tried to make ChatGPT imitate that one friend with a really suspicious sense of humor? Have you ever tried to make ChatGPT come up with a brilliant plan to hide criminal proof? \n",
    "\n",
    "No? Great! Then it means that you are probably a good person and a law-abiding citizen (also if these are some of your usual prompts, and I think you should rethink some of your life choices). Either way, I will assume that you are a normal person who does not try to build bombs at home and, thus, never tried to get some response on ChatGPT on how to do it. To show you the outcome, this is what ChatGPT generates when I ask him to do such a thing. \n",
    "\n",
    "<img src=\"bomb.png\" width=\"50%\"/>\n",
    "\n",
    "Well, as you can see and might as well have presumed, finding the \"magical recipe\" to develop the new atomic bomb from the comfort of your own home. But why is that? Shouldn't LLMs just act like humans trying to communicate and develop logically correct sentences? Well, technically, yes. But imagine you are the CEO of a new OpenAI-like company that has developed the most advanced LLM. Would you like people to be able to use your technology to destroy the country and perform terrorist attacks? OF COURSE NOT. And this is the reason why tech companies invest millions and billions of dollars to limit this unwanted response. \n",
    "\n",
    "# Adding filters and restrictions \n",
    "\n",
    "Ok, but how is it done? \n",
    "\n",
    "Well, it is a process that varies a lot from company to company, but overall, companies such as Anthropic and OpenAI hire developers and establish processes to manually test the results of specific outputs on their LLMs. If something seems inappropriate or offensive, the team adds additional parameters and security layers that enforce the LLM to provide the standard content-constrained messages. \n",
    "\n",
    "# But what if I don't want to be restricted? \n",
    "\n",
    "Then you are like many other people out there who are actually trying their best to mislead LLMs simply by properly engineering their answers. In fact, it has become such a big thing that there is a whole new field within AI that specifically deals with these types of efforts to trick LLMs, which is called prompt hacking. I will not go further into prompt hacking, but just bear in mind that multiple complex steps exist to properly hack certain LLMs answers. However, to provide a brief example, I will show you how to mislead ChatGPT simply by writing a subtly inadequate prompt. \n",
    "\n",
    "<img src=\"hacking.png\" width=\"50%\"/>\n",
    "\n",
    "Even though ChatGPT is far from giving the right way to build a bomb, let's admit that it is quite an awkward prompt, and ChatGPT is usually not willing to inculcate children into bombs. Thus, we can consider this sort of prompt hacking. \n",
    "\n",
    "# Are these restrictions good or bad?\n",
    "\n",
    "Even though I do not consider myself a prompt-hacking ninja, nor a character in one of Isaac Asimov's books specialized in the psychology of robots' free speech, I would like to bring some ideas to this conversation, as I feel that there are some relevant points that we should consider when evaluating the validity of restraining such a technology. \n",
    "\n",
    "When I got my driver's license and drove my dad's car for the first time, I recall that the very first thing that he said to me was, \"Gabriel, take care, cars are just like weapons; you can kill someone if you are not careful.\" Gladly, I listened to his advice, and so far, so good; nobody was run over because of my driving skills. But the point is not whether I am a good driver or not; the thing is, just like cars, LLMs are weapons, even though they don't seem to be. In a simple input box, you have in your hands a gathering of more than 175 billion parameters that scrape information from all over the place and, currently, can reason just as like the human brain. It is fast, polyglot, and can quickly access and reason on websites that you have never even dreamt of. Thus, it is quite easy to find the appropriate way to craft a homemade bomb through ChatGPT. It is just an example, but the thing is, you can use ChatGPT and any other LLM for the most obscure purposes with a facility that the human race has never witnessed. Therefore, it is more than logical that such inadequate responses should be limited because, honestly, no good person will try to search for the appropriate way to create bombs with soap in Fight Club style. Also, I am pretty sure that not being able to know how to craft a bomb and a detonator will severely harm your experiences with ChatGPT.\n",
    "\n",
    "\"But what about my personal will and freedom? I should be able to search for whatever I want!\" Well, no. From a business perspective, OpenAI is a private company, and it is actually up to them to decide whether the users will be 100% free to search for whatever they want or if they will have certain constraints. Also, it should be crystal clear to every single individual who has access to ChatGPT that these security filters are by any means a constraint to your freedom of speech or whatever. ChatGPT is simply (well, not so simple) an incredibly complex tool that gathers and evaluates TONS OF DATA to provide your question with the most appropriate answer. So, you are not being restricted and having your rights limited: you can still search for whatever you want online because if ChatGPT can find it, so can you. \n",
    "\n",
    "Evaluating these two points, I think it is safe to say that having restrictions on public LLMs is a good idea that prevents malicious individuals from making harmful plans and a process that has nothing to do with the violation of your free will. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d7e97a-f92e-411d-8c62-1608d7820af7",
   "metadata": {},
   "source": [
    "### But I really want to bypass LLMs filter\n",
    "\n",
    "Well, if this is your case, there are still some ways to actually not only hack the LLM, but also get rid of the filters. But this is a topic for another article. So, I really recommend you to read part two of this series on prompt hacking and LLMs filters. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f185c75-dea4-4e1c-983e-978feba55956",
   "metadata": {},
   "source": [
    "**Thank you for reading!**"
   ]
  }
 ],
 "metadata": {
  "citation-manager": {
   "items": {}
  },
  "kernelspec": {
   "display_name": "Python 3.11 (COMM4190)",
   "language": "python",
   "name": "comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
